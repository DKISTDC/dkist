.. _loadinglevel1data:

Loading and Working with Level One Data
=======================================

As we saw in the :ref:`downloading-fits` section, once we have an ASDF file representing a DKIST dataset it can be loaded with `dkist.Dataset`.
The `dkist.Dataset` class provides access to all the components of the dataset, and is a slightly customised `ndcube.NDCube` object, so all functionality provided by ndcube is applicable to the ``Dataset`` class.

This section of the guide will cover things specifc to the `~dkist.Dataset` class, but will not cover the basics which are in the `ndcube documentation <https://docs.sunpy.org/projects/ndcube>`__.
In particular we recommend you read the following sections of the ``ndcube`` documentation:

* :ref:`ndcube:ndcube`
* :ref:`ndcube:coordinates`
* :ref:`ndcube:slicing`

before coming back to this guide.

The Components of a `~dkist.Dataset`
------------------------------------

Before we dive in, let us look at the core components of the `~dkist.Dataset` object and what each of them provides.
All of these components are available when an ASDF file is loaded, even if no FITS files have been downloaded, however, no data will be available without the FITS files (all pixels will be NaN).

* The `~dkist.Dataset.data` property is a `dask.array.Array` object which lazy loads the array data from the FITS files on demand.
* The `~dkist.Dataset.wcs` property is a `gwcs.wcs.WCS` object which provides coordinate information for all the pixels in the array.
* The `~dkist.Dataset.files` property is an instance of the `dkist.io.FileManager` class which allows you to access and modify information about how the FITS files are loaded by the array.
* The `~dkist.Dataset.headers` property is an `astropy.table.QTable` instance which is a complete record of all the FITS headers for all the FITS files in the dataset. This provides access to all the available metadata about the dataset without having to retrieve or open any of the FITS files.

The relationship between `~dkist.Dataset.data` and `~dkist.Dataset.files`
#########################################################################

When an ASDF file containing a dataset is loaded, a new array is generated from the list of FITS filenames provided in that ASDF file.
These FITS filenames are ordered in the correct way so that an array of the correct dimensionality is created. This array represents the whole dataset as a higher dimensional array than is contained in any single FITS file.
Depending on how the ASDF file was obtained, you may or may not have some or all of the corresponding FITS files.
If any of the FITS files referenced by the ADSF file are in the same directory as the ASDF file they will be opened automatically when their slice of the array is requested.

The `~dkist.io.FileManager` object is provided to let you inspect and modify how this loading occurs.
The two main functions it enables are: modifying the base path for where the FITS files are loaded from, and transferring FITS files using Globus.
The `~dkist.io.FileManager.basepath` property sets the directory from which the FITS files will be loaded on demand; assigning this to a different directory will cause the FITS to be loaded from that path.
If at any point the file with the required filename as specified by the ASDF can not be found its slice of the array will be returned as NaN.
The `~dkist.io.FileManager.download` method is designed to allow you to transfer some or all of the FITS files from the data center using Globus.
For more details on how to use it see the :ref:`downloading-fits` section.
It is important to note that `~dkist.io.FileManager.download` will set the `~dkist.io.FileManager.basepath` property to the destination directory when using a local personal endpoint.

The Dask Array
##############

As mentioned above, the array associated with a `dkist.Dataset` object is a `Dask Array <https://docs.dask.org/en/latest/array.html>`__.
Dask allows you to delay, and distribute computation over a wide variety of different types of processors to support larger than memory arrays and very compute intensive applications.
The `dkist` package uses Dask to delay the opening of the FITS files containing the data until that section of the array is needed.

If you want to make use of any of the features of the dask array, it is recommended you read `the Dask documentation <https://docs.dask.org/en/latest/array.html>`__.

Chunking
~~~~~~~~

Dask arrays are split into chunks.
Each chunk will be loaded as a whole and when Dask distributes an array across many processors, each chunk will be sent to a single process.
The Dask array generated by the Python tools has one chunk per FITS file.

If you need to do advanced computation on the array, you may need to `rechunk <https://docs.dask.org/en/latest/array-chunks.html#rechunking>`__ the array.
For example if you wanted to perform a fitting operation along the wavelength axis, you may want one chunk per pixel for each wavelength.
This would allow a much faster distributed computation of the fit, but at the expense of memory to load and rechunk the array.

Slicing and files
-----------------

As described in ndcube's :ref:`ndcube:slicing` documentation, using the standard Python slicing syntax you can reduce the size of the dataset by indexing in array space.
Slicing works the same way for the `~dkist.Dataset` object as it does for `~ndcube.NDCube`, we shall not cover the details here.
The only thing which is specific to the `~dkist.Dataset` class is the interaction between the slicing syntax and the `~dkist.Dataset.files` property.

When you slice a dataset the new, smaller, dataset has a new `~dkist.Dataset.files` object which is unrelated to the one of the larger parent `~dkist.Dataset`.
This means that if you slice the dataset::

  >>> ds = dkist.Dataset.from_asdf(myfilename)
  >>> small_ds = ds[10:20, :, 5]

and then download the files corresponding to the smaller dataset::

  >>> small_ds.download()

The data will be available for the smaller dataset and not the larger one, as only ``small_ds.files.basepath`` is modified by ``small_ds.files.download`` and not ``ds.files.basepath``.
To set the parent dataset to use the same basepath as the post-download smaller dataset you have to run::

  >>> ds.files.basepath = small_ds.files.basepath
